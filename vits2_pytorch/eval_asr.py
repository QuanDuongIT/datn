import os
import re
import csv
from infer_onnx import infer_long_text
import sys
sys.path.append('../eval_asr') 
from eval_audio import evaluate_tts_with_asr
import IPython.display as ipd
from get_loss import LogAnalyzer

def synthesize_and_evaluate(
    text_path,
    model_path,
    config_path,
    audio_output_path="/content/audio.wav",
    whisper_model_size="medium",
    sampling_rate=16000,
    csv_path=None
):
    from get_loss import LogAnalyzer  # ƒê·∫£m b·∫£o ƒë√£ import
    import csv

    with open(text_path, "r", encoding="utf-8") as file:
        text = file.read()

    # T·ªïng h·ª£p gi·ªçng n√≥i
    infer_long_text(
        content=text,
        model_path=model_path,
        config_path=config_path,
        output_path=audio_output_path
    )

    # ƒê√°nh gi√° b·∫±ng Whisper
    result = evaluate_tts_with_asr(audio_output_path, text, whisper_model_size)
    text_clean = text.strip().replace('\n', ' ')

    print(f'\n| {whisper_model_size.upper()} |{"-"*80}')
    print(f"VƒÉn b·∫£n g·ªëc         : {text_clean}")
    print(f"VƒÉn b·∫£n ASR         : {result['asr_text']}")
    print(f"Phoneme g·ªëc         : {result['ground_truth_phonemes']}")
    print(f"Phoneme t·ª´ ASR      : {result['asr_text_phonemes']}")
    print(f"Word Error Rate     : {result['wer']*100:.2f}%")

    log_dir = os.path.dirname(os.path.dirname(model_path))
    step = None
    loss_data = {
        "loss_g_total": None,
        "loss_d_total": None,
        "loss_mel": None,
        "loss_fm": None,
        "loss_g_kl": None
    }

    # Tr√≠ch xu·∫•t step
    match = re.search(r'G_(\d+)\.onnx', os.path.basename(model_path))
    if match:
        step = int(match.group(1))
        try:
            analyzer = LogAnalyzer(log_dir)
            loss_data = analyzer.search_logs_by_checkpoint(step)
        except Exception as e:
            print(f"L·ªói khi ph√¢n t√≠ch log: {e}")
    else:
        print("Kh√¥ng th·ªÉ x√°c ƒë·ªãnh step t·ª´ t√™n file m√¥ h√¨nh.")

    # T·∫°o file CSV n·∫øu ch∆∞a c√≥
    if csv_path is None:
        csv_path = os.path.join(log_dir, "metrics.csv")

    if step is not None:
        file_exists = os.path.exists(csv_path)
        with open(csv_path, mode='a', newline='', encoding='utf-8') as csvfile:
            fieldnames = ["step", "wer"] + list(loss_data.keys())
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            if not file_exists:
                writer.writeheader()
            writer.writerow({
                "step": step,
                "wer": result["wer"],
                **loss_data
            })

    return {
        **result,
        "audio": ipd.Audio(audio_output_path, rate=sampling_rate)
    }

def evaluate_all_checkpoints(
    models_dir,
    text_path,
    config_path,
    whisper_model_size="medium"
):
    """
    Ch·∫°y ƒë√°nh gi√° cho c√°c m√¥ h√¨nh G_*.onnx trong th∆∞ m·ª•c c√≥ step chia h·∫øt cho 5000.

    Args:
        models_dir (str): Th∆∞ m·ª•c ch·ª©a c√°c file G_*.onnx.
        text_path (str): File vƒÉn b·∫£n ƒë·∫ßu v√†o.
        config_path (str): File config.
        whisper_model_size (str): K√≠ch th∆∞·ªõc Whisper model.
    """
    model_files = []
    for f in os.listdir(models_dir):
        match = re.match(r'G_(\d+)\.onnx', f)
        if match:
            step = int(match.group(1))
            if step % 5000 == 0:
                model_files.append((step, os.path.join(models_dir, f)))

    # S·∫Øp x·∫øp theo step tƒÉng d·∫ßn
    model_files.sort(key=lambda x: x[0])

    print(f"üîç Ph√°t hi·ªán {len(model_files)} m√¥ h√¨nh h·ª£p l·ªá (chia h·∫øt cho 5000) ƒë·ªÉ ƒë√°nh gi√°...")

    for step, model_path in model_files:
        print(f"\nüîß ƒê√°nh gi√° m√¥ h√¨nh: {model_path}")
        synthesize_and_evaluate(
            text_path=text_path,
            model_path=model_path,
            config_path=config_path,
            whisper_model_size=whisper_model_size
        )

